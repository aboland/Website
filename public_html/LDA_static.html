<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Aidan Boland" />

<meta name="date" content="2015-10-16" />

<title>LDA</title>

<script src="libs/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.1/css/cerulean.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">

/* padding for bootstrap navbar */
body {
  padding-top: 50px;
  padding-bottom: 40px;
}


/* offset scroll position for anchor links (for fixed navbar)  */
.section h2 {
  padding-top: 55px;
  margin-top: -55px;
}
.section h3 {
  padding-top: 55px;
  margin-top: -55px;
}



/* don't use link color in navbar */
.dropdown-menu>li>a {
  color: black;
}

/* some padding for disqus */
#disqus_thread {
  margin-top: 45px;
}

</style>

<link rel="stylesheet" href="libs/font-awesome-4.1.0/css/font-awesome.min.css"/>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="libs/highlight/default.css"
      type="text/css" />
<script src="libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>


<link rel="stylesheet" href="mystyle.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Aidan Boland</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li><a href="/">Home</a></li>
        <li class="dropdown">
          <a href="authoring" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">Links <span class="caret"></span></a>
          <ul class="dropdown-menu" role="menu">
             <li class="dropdown-header">Documents</li>
             <li><a href="/LDA">LDA</a></li>
             <li><a href="/LDA/LDA.pdf">LDA [PDF]</a></li>
             <!--<li><a href="blank.html">Blank</a></li>-->
             <li class="divider"></li>
             <li class="dropdown-header">Sites</li>
             <li><a href="http://ff.aboland.ie">Premier League Stats</a></li>
          </ul>
        </li>
        <li><a href="blank.html">Test page</a></li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">
<h1 class="title">LDA</h1>
<h4 class="author"><em>Aidan Boland</em></h4>
<h4 class="date"><em>2015-10-16</em></h4>
</div>


<div id="latent-dirichlet-allocation" class="section level1">
<h1>Latent Dirichlet Allocation</h1>
<p>This documents is a summary of Latent Dirichlet Allocation, a method first presented in <a href="https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">Blei et al., 2002</a>.</p>
<div id="notation" class="section level2">
<h2>Notation</h2>
<p>The data is composed as follows,</p>
<ul>
<li>A vocabulary of <span class="math">\(V\)</span> possible words <span class="math">\((w^1,..., w^V)\)</span>.</li>
<li>A corpus of <span class="math">\(M\)</span> individual documents, <span class="math">\({\bf D} = ({\bf w}_1,{\bf w}_2,..., {\bf w}_M)\)</span>.</li>
<li>Document <span class="math">\(d\)</span> is composed of <span class="math">\(N_d\)</span> words, <span class="math">\({\bf w}_d = (w_{d,1}, w_{d,2},..., w_{d,N_d})\)</span>.</li>
<li>There are <span class="math">\(k\)</span> topics <span class="math">\((1,...,k)\)</span>.</li>
</ul>
<p>For consitency the following letters will be used as sub and super scripts,</p>
<ul>
<li><span class="math">\(i\)</span> for topic <span class="math">\((1,...,k)\)</span>.</li>
<li><span class="math">\(j\)</span> for word in vocabulary <span class="math">\((1,...,V)\)</span>.</li>
<li><span class="math">\(n\)</span> for word in document <span class="math">\((1,...,N_d)\)</span>.</li>
<li><span class="math">\(d\)</span> for document <span class="math">\((1,...,M)\)</span>.</li>
</ul>
</div>
<div id="generating-a-document" class="section level2">
<h2>Generating a document</h2>
<p>The following steps show how document <span class="math">\({\bf w}_d\)</span> is generated.</p>
<ol style="list-style-type: decimal">
<li>Choose the number of words in the document, <span class="math">\(N_d\sim Po(\xi)\)</span>.</li>
<li>Choose the probabilities of the topics for the document, <span class="math">\(\theta_d\sim Dir(\alpha)\)</span>.    (<span class="math">\(\theta\)</span> is a <span class="math">\(1\times k\)</span> vector)</li>
<li>For each of the <span class="math">\(N_d\)</span> words <span class="math">\(w_{d,n}\)</span>,
<ul>
<li>Choose a topic <span class="math">\(z_{d,n}\sim Mult(\theta_d)\)</span>,    (<span class="math">\(z_{d,n}\)</span> is a single value from <span class="math">\(1\)</span> to <span class="math">\(k\)</span>, the topic of word <span class="math">\(n\)</span> in doc <span class="math">\(d\)</span>)</li>
<li>Choose a word <span class="math">\(w_{d,n}\sim Mult(\phi_{z_{d,n}})\)</span>,</li>
</ul></li>
</ol>
<p>where <span class="math">\(\phi\sim Dir(\beta)\)</span>. The choices of the hyperparameters <span class="math">\(\alpha\)</span> and <span class="math">\(\beta\)</span> will be discussed further in the next section. Figure  shows a graphical representation of LDA.</p>

<div id="parameters" class="section level3">
<h3>Parameters</h3>
<p>The word probabilities are paramterised by the <span class="math">\(k\times V\)</span> matrix <span class="math">\(\phi\)</span>, <span class="math">\[
P(w_{d,n}=j|z_{d,n}=i) = \phi_{j,i}
\]</span> The topic probabilities for each document are paramterised by the vector <span class="math">\(\theta_d\)</span> <span class="math">\[
P(z_{d,n}=j) = \theta_{d,j}
\]</span> Latent dirichlet as presented in <a href="https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">Blei et al., 2002</a> is a problem of maximising the following, <span class="math">\[
P(D|\phi,\alpha) = \int P(D|\phi,\theta)P(\theta|\alpha)d\theta,
\]</span> this can be achieved using variational inference an the EM algorithm. (note, in the Blei paper they do not put a prior on <span class="math">\(\phi\)</span>)</p>
</div>
<div id="gibbs-sampling" class="section level3">
<h3>Gibbs sampling</h3>
<p>An alternative efficient procedure is <a href="http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf">collapsed Gibbs sampling</a> (<a href="http://www.datalab.uci.edu/papers/fastLDA_2008.pdf">alternative paper</a>) where <span class="math">\(\theta\)</span> and <span class="math">\(\phi\)</span> are marginalised out and only the latent variables <span class="math">\({\bf z}\)</span> are sampled. After the sampler has burned-in we can calculate an estimate of <span class="math">\(\theta\)</span> and <span class="math">\(\phi\)</span> given <span class="math">\({\bf z}\)</span>.</p>
<p>The posterior distribution of interest in this case is, <span class="math">\[
P({\bf z}|D) = \dfrac{P(D, {\bf z})}{\sum_{\bf z}P(D, {\bf z})} \propto P(D|{\bf z})P({\bf z})
\]</span></p>
<p>The joint distribution, <span class="math">\(P(D, {\bf z})\)</span>, can be found through integrating <span class="math">\[P(\theta,\phi,{\bf z}, D|\alpha,\beta)=P(D|{\bf z},\phi)P(\phi|\beta)P({\bf z}|\theta)P(\theta|\alpha)\]</span></p>
<p>with respect to <span class="math">\(\phi\)</span> and <span class="math">\(\theta\)</span>.<br />Integrating with repsect to <span class="math">\(\phi\)</span> gives the following,</p>
<p><span class="math">\[
p(D|{\bf z}) = 
    \prod_{i=1}^K \left[ \dfrac{\Gamma\left(\sum_{j=1}^V\beta_{i,j}\right)}{\prod_{j=1}^V\Gamma(\beta_{i,j})}\times
\dfrac{\prod_{j=1}^V \Gamma\left(C^i_{j,\cdot}+\beta_{i,j}\right)}{\Gamma\left(\sum_{j=1}^V C^i_{j,\cdot}+\beta_{i,j}\right)}\right]\;\;\;(1)
\]</span> while integrating with respect to <span class="math">\(\theta\)</span> gives, <span class="math">\[
p({\bf z}) = \prod_{d=1}^M\left[ \dfrac{\Gamma\left(\sum_{j=1}^k\alpha_j\right)}{\prod_{j=1}^k\Gamma(\alpha_j)}\times
\dfrac{\prod_{j=1}^k\Gamma\left(C^\cdot_{j,d}+\alpha_j\right)}{\Gamma\left(\sum_{j=1}^k(C^\cdot_{j,d}+\alpha_j)\right)}\right]\;\;\;(2)
\]</span></p>
<p>Where <span class="math">\(C^{i,-(d,n)}_{j,d}\)</span> is the count of words <span class="math">\(w^j\)</span> that have topic <span class="math">\(i\)</span> in document <span class="math">\({\bf w}_d\)</span> not including the word <span class="math">\(w_{d,n}\)</span>. The notation <span class="math">\(\cdot\)</span> is a sum over that subscript,<br />e.g. <span class="math">\(C^{i,-(d,n)}_{j,\cdot}\)</span> is the number of words <span class="math">\(w^j\)</span> that are assigned to topc <span class="math">\(j\)</span> in the full corpus not including the word <span class="math">\(w_{d,n}\)</span>.</p>
<p>The full conditional distribution <span class="math">\(P(z_{d,n} = i|{\bf z_{-(d,n)}}, w_{d,n}=j, D, \alpha, \beta)\)</span> can be found using equations (1) and (2), <span class="math">\[
P(z_{d,n} = i|{\bf z_{-(d,n)}}, w_{d,n}=j, D, \alpha, \beta) \propto \dfrac{\beta_{i,j}+C^{i,-(d,n)}_{j,\cdot}}{\beta_{i,\cdot} + C^{i,-(d,n)}_{\cdot,\cdot}} \dfrac{\alpha_i+C^{i,-(d,n)}_{\cdot,d}}{\alpha_\cdot+C^{\cdot,-(d,n)}_{\cdot,d}}
\]</span> The labels in the corpus can be updated with Gibbs sampling using full conditional distribution.</p>
</div>
<div id="implementation" class="section level3">
<h3>Implementation</h3>
<p>The following code can be used to update the labels using the collapsed Gibbs sampler as described above. The first function creates documents given a set of priors.</p>
<pre class="r"><code>
library(MCMCpack)  # for dirichlet dist, alternative is library(gtools)

GenerateLDA &lt;- function(alpha, beta, xi, Vocab, Ndoc){
   # function to generate a single document
   # arguments:
   #   alpha, k x 1 concentration paramter for topics,(take input as single value for now)
   #   beta, k x V each row represents the concentration parameter for words in topic k
   #   xi, parameter to simulate length of documents
   #   Vocab, vector of size V containing all possible words
   #   ndoc, number of documents to simulate
   #
   # todo:
   #   change output to document term matrix rather than list....
  
   alpha &lt;- rep(alpha, nrow(beta))
  
   if(dim(beta)[2] != length(Vocab))
      stop(&quot;Incorrect dimension for Beta!!&quot;)
  
   docs &lt;- list()  # blank list to store words, documents diff size so array 
                   # is not suitable, could use DTM alternatively...
   z_out &lt;- NULL
   for(i in 1:Ndoc){
      N &lt;- rpois(1, lambda = xi)  # Choose the length of the ith doc
      theta &lt;- rdirichlet(n = 1, alpha = alpha)
    
      z &lt;- rmultinom(1, 1, prob = theta)  # Choose a topic for 1st word in doc i
      z_out &lt;- c(z_out, which(z == 1))
      phi &lt;- rdirichlet(n = 1, alpha = beta[which(z == 1),])  # generate probs
      docs[[i]] &lt;- Vocab[which(rmultinom(1, 1, prob = phi) == 1)]  # Initialise ith doc
    
      for(j in 2:N){
         z &lt;- rmultinom(1, 1, prob = theta)  # Choose a topic for jth word in doc i  
         z_out &lt;- c(z_out, which(z == 1))
         phi &lt;- rdirichlet(n = 1, alpha = beta[which(z == 1),])
         docs[[i]] &lt;- c(docs[[i]], Vocab[which(rmultinom(1, 1, prob = phi) == 1)])
      }
   }
   list(docs = docs, z = z_out)
}

my_topic_counts &lt;- function(dtm, z, n_topics){
  # Function to calculate topic counts for words and documents
  # arguments:
  #   dtm is a document term matrix in triplet form
  #     dtm$i denotes the document
  #     dtm$j denotes the word
  #     dtm$v is the number of occurences of word j in doc i
  #   z is a set of topic labels for each word
  
  word &lt;- rep(dtm$j,dtm$v)
  doc &lt;- rep(dtm$i,dtm$v)
  
  word_count &lt;- array(0,c(n_topics,ncol(dtm)))
  doc_count &lt;- array(0,c(n_topics,nrow(dtm)))
  
  for(w in 1:length(doc)){
      word_count[z[w],word[w]] &lt;- word_count[z[w],word[w]] + 1
      doc_count[z[w],doc[w]] &lt;- doc_count[z[w],doc[w]] + 1
  }
  list(word_count, doc_count)
}

my_lda_gibbs &lt;- function(dtm, n_topic = 2, iterations = 10){
  
   n_vocab &lt;- ncol(dtm)  # Number of unique words
   alpha &lt;- rep(1, n_topic)  # prior on theta
   beta &lt;- array(1,c(n_topic, n_vocab))  # prior on phi
   theta &lt;- array(0,c(nrow(dtm),n_topic))
   phi &lt;- array(0,c(n_topic, n_vocab))
  
   doc &lt;- rep(dtm$i,dtm$v)
   word &lt;- rep(dtm$j,dtm$v)
  
   z &lt;- array(0, c(iterations + 1, length(doc)))
   z[1, ] &lt;- sample(1:n_topic, length(doc), replace = TRUE)  # choose initial labels
   if(length(unique(z[1, ])) != n_topic)
      stop(&quot;Number of topics in z vector do not match argument!\n&quot;)
  
   initial_counts &lt;- my_topic_counts(dtm, z[1, ], n_topics =n_topic)
   word_count &lt;- initial_counts[[1]]
   doc_count &lt;- initial_counts[[2]]
   topic_prob &lt;- array(0, n_topic)
  
   for(iter in 1:iterations){
      for(w in 1:length(word)){
         # Loop over each word
         for(j in 1:n_topic){
            if(z[iter, w] == j){
                topic_prob[j] &lt;- prod(sum(beta[j, word[w]], word_count[j, word[w]], -1), 
                                    sum(alpha[j], doc_count[j, doc[w]], -1)) / 
                  prod(sum(beta[j, ], word_count[j, ], -1), 
                     (sum(alpha, doc_count[, doc[w]], -1)))
            }else{
                topic_prob[j] &lt;- prod(sum(beta[j, word[w]], word_count[j, word[w]]), 
                              sum(alpha[j], doc_count[j, doc[w]])) / 
                  prod(sum(beta[j, ], word_count[j, ]), 
                     sum(alpha, doc_count[, doc[w]]))
            }
         }
         z[iter + 1, w] &lt;- sample.int (n_topic, size = 1, prob = topic_prob)
         
         if(z[iter + 1, w] != z[iter, w]){
            word_count[z[iter, w], word[w]] &lt;- word_count[z[iter, w], word[w]] - 1
            doc_count[z[iter, w], doc[w]] &lt;- doc_count[z[iter, w], doc[w]] - 1
            word_count[z[iter + 1, w], word[w]] &lt;- word_count[z[iter + 1, w], word[w]] + 1
            doc_count[z[iter + 1, w], doc[w]] &lt;- doc_count[z[iter + 1, w], doc[w]] + 1
         }
      }
      # Old code when counts updated after each full sweep of words
      #update_counts &lt;- my_topic_counts(dtm, z[iter + 1, ], n_topic)
      #word_count &lt;- update_counts[[1]]
      #doc_count &lt;- update_counts[[2]]
  }
  for(j in 1:n_topic)
     phi[j, ] &lt;- (beta[j, ] + word_count[j, ]) / (sum(beta[j, ]) + sum(word_count[j, ]))
  for(d in 1:nrow(dtm))
     theta[d, ] &lt;- (alpha + doc_count[, d]) / (sum(alpha) + sum(doc_count[, d]))
  
  list(z, t(phi), theta)
}</code></pre>
</div>
<div id="testing-the-functions" class="section level3">
<h3>Testing the functions</h3>
<p>We can now implement these functions using a sample set of words. The corpus was created using a specific set of <span class="math">\(\beta\)</span> prior values.</p>
<pre class="r"><code>my_vocab &lt;- c(&quot;car&quot;, &quot;engine&quot;, &quot;exhaust&quot;, &quot;wheel&quot;,
              &quot;milk&quot;, &quot;cream&quot;, &quot;dairy&quot;, &quot;yogurt&quot;,
              &quot;coke&quot;, &quot;water&quot;, &quot;juice&quot;, &quot;coffee&quot;, &quot;drink&quot;, &quot;bottle&quot;, &quot;can&quot;)
n_topics &lt;- 3
# my_beta &lt;- matrix(1, nrow = n_topics, ncol = length(my_vocab))
my_beta &lt;- matrix(c(10, 10, 10, 10, 01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
                    0.1, 0.1, 0.1, 0.1, 10, 10, 10, 10, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,
                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 10, 10, 10, 10, 10, 10, 10), 
                  byrow = T, nrow = n_topics, ncol = length(my_vocab))
my_corpus &lt;- GenerateLDA(alpha = 1, beta = my_beta, xi = 10, Vocab = my_vocab, Ndoc = 100)
library(tm)  # Library to create DTM
my_dtm &lt;- DocumentTermMatrix(Corpus(VectorSource(my_corpus$docs)))</code></pre>
<p>The corpus was then analysed using a pre-built function from the <em><code>topicmodels</code></em> package (<code>LDA</code>) and the function created above (<code>my_lda_gibbs</code>). The priors for the Gibbs steps were left vague.</p>
<pre class="r"><code>library(topicmodels)
time_pacakge &lt;- system.time(
  control_gibbs &lt;- LDA(my_dtm, k = 3, method=&quot;Gibbs&quot;, iterations = 3000, burn.in=1000)
)

time_mine &lt;- system.time(
  my_gibbs &lt;- my_lda_gibbs(my_dtm, n_topic = 3, iterations = 3000)
)</code></pre>
<p>The <code>LDA</code> function took 0.1 seconds to run while the <code>my_lda_gibbs</code> function took 80.19 seconds. The <code>LDA</code> function is optimised to run in <code>c</code>, this is where it gains it’s time advantage. One problem is that neither of the functions seem to converge well. The tables below compare the 2 algorithms with the true values for each word in the corpus.</p>
<pre class="r"><code>my_gibbs_z &lt;- apply(my_gibbs[[1]][-1000, ], 2, median)
table(z = my_corpus$z, my_gibbs = my_gibbs_z)</code></pre>
<pre><code>   my_gibbs
z     1   2   3
  1 172  56  76
  2  60 197  63
  3  67  73 189</code></pre>
<pre class="r"><code>table(z = my_corpus$z, control_gibbs@z)</code></pre>
<pre><code>   
z     1   2   3
  1  57  81 166
  2 191  68  61
  3  73 186  70</code></pre>
<pre class="r"><code>table(package_gibbs = control_gibbs@z, my_gibbs = my_gibbs_z)</code></pre>
<pre><code>             my_gibbs
package_gibbs   1   2   3
            1  17 301   3
            2   9  10 316
            3 273  15   9</code></pre>
<p>Seems to do ok, not fantastic. There may be a label matching problem but this not too big of a deal, it is a standard problem in clustering. What is nice is that both algorithms seem to get the same results (last table). This indicates that the functions created in this document are correct.</p>
</div>
</div>
</div>
<div id="seeded-topics" class="section level1">
<h1>Seeded topics</h1>
<p>One simple solution to incorporating seed words into the LDA model is presented in <a href="http://www.umiacs.umd.edu/~jags/pdfs/GuidedLDA.pdf">Jagarlamundi et. al ?</a>. A set of seed words can be provded by the user, these are used to help the model learn about the topics. Jagarlamundi et. al build a model which uses the seed words to improve both the topic-word and document-topic probability distributions. Following the paper both of these models are presented separately first (Model 1 and Model 2) and then combined.</p>
<div id="word-topic-distributions-model-1" class="section level2">
<h2>Word-Topic Distributions (Model 1)</h2>
<p>This model chooses words from two Multinomial distributions: a <code>seed topic' distribution and a</code>regular topic’ distribution. The seed topic distribution is constrained to only generate words from a corresponding seed set. The regular topic distribution may generate any word including seed words.</p>
<ol style="list-style-type: decimal">
<li>Choose regular topic <span class="math">\(\phi^r_k\sim Dir(\beta_r)\)</span></li>
<li>Choose seed topic <span class="math">\(\phi^s_k\sim Dir(\beta_s)\)</span></li>
<li>Choose <span class="math">\(\pi_k\sim Beta(1,1)\)</span></li>
</ol>
<p>Document <span class="math">\({\bf w}_d\)</span> is then generated as follows.</p>
<ol style="list-style-type: decimal">
<li>Choose the number of words in the document, <span class="math">\(N_d\sim Po(\xi)\)</span>.</li>
<li>Choose the probabilities of the topics for the document, <span class="math">\(\theta_d\sim Dir(\alpha)\)</span>.    (<span class="math">\(\theta\)</span> is a <span class="math">\(1\times k\)</span> vector)</li>
<li>For each of the <span class="math">\(N_d\)</span> words <span class="math">\(w_{d,n}\)</span>,
<ul>
<li>Choose a topic <span class="math">\(z_{d,n}\sim Mult(\theta_d)\)</span>,    (<span class="math">\(z_{d,n}\)</span> is a single value from <span class="math">\(1\)</span> to <span class="math">\(k\)</span>, the topic of word <span class="math">\(n\)</span> in doc <span class="math">\(d\)</span>)</li>
<li>Select an indicator <span class="math">\(x_{d,n}\sim Bern(\pi_{z_{d,n}})\)</span></li>
<li>If <span class="math">\(x_{d,n}=0\)</span> choose a word <span class="math">\(w_{d,n}\sim Mult(\phi^s_{z_{d,n}})\)</span>,</li>
<li>If <span class="math">\(x_{d,n}=1\)</span> choose a word <span class="math">\(w_{d,n}\sim Mult(\phi^r_{z_{d,n}})\)</span>,</li>
</ul></li>
</ol>
<p>where <span class="math">\(\phi\sim Dir(\beta)\)</span>. The choices of the hyperparameters <span class="math">\(\alpha\)</span> and <span class="math">\(\beta\)</span> will be discussed further in the next section. Figure  shows a graphical representation of LDA.</p>

</div>
<div id="document-topic-distributions-model-2" class="section level2">
<h2>Document-Topic Distributions (Model 2)</h2>
<p>Very high level idea is that documents which contain a word from one of the seed topics have a higher probability of coming from this topic…..sounds a bit simple</p>
<ol style="list-style-type: decimal">
<li>For each topic choose <span class="math">\(\phi_j\sim Dir(\beta)\)</span></li>
<li>For each seed topic choose a group-topic distribtuion <span class="math">\(\psi_j\sim Dir(\alpha)\)</span></li>
</ol>
<p>Document <span class="math">\({\bf w}_d\)</span> is then generated as follows.</p>
<ol style="list-style-type: decimal">
<li>Choose binary vector <span class="math">\(b\)</span> (vector of length K indicating whether seed topic j is in document)
<ul>
<li>when generating documents this can be choosen randomly</li>
</ul></li>
<li>Choose document-group distribution <span class="math">\(\xi_d\sim Dir(\tau b)\)</span></li>
<li>Choose a group varaible <span class="math">\(g\sim Mult(\xi_d)\)</span></li>
<li>Choose <span class="math">\(\theta_d\sim Dir(\psi_g)\)</span></li>
<li>For each of the <span class="math">\(N_d\)</span> words <span class="math">\(w_{d,n}\)</span>,
<ul>
<li>Choose a topic <span class="math">\(z_{d,n}\sim Mult(\theta_d)\)</span>,</li>
<li>Choose a word <span class="math">\(w_{d,n}\sim Mult(\phi_{z_{d,n}})\)</span>.</li>
</ul></li>
</ol>

</div>
</div>
<div id="hierarchical-lda" class="section level1">
<h1>Hierarchical LDA</h1>
<p>Hierarchically Supervised Latent Dirichlet Allocation <a href="http://people.dbmi.columbia.edu/noemie/papers/nips11.pdf">Perotte et. al 11</a>.</p>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
